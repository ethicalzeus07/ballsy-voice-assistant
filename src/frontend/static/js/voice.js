/**
 * Complete Voice Assistant with Speech Recognition AND Speech Synthesis
 * Now with "Ballsy" personality and voice responses
 *
 * This version uses ‚Äúone‚Äêshot‚Äù (non‚Äêcontinuous) recognition, so it only fires
 * a single final result when you stop speaking. It no longer auto‚Äêrestarts itself.
 */


// Configuration
const SPEECH_TIMEOUT = 10000; // (not actively used)
const CLICK_DEBOUNCE = 2000;  // 2 seconds between orb clicks

// Determine API and WebSocket endpoints dynamically based on the page origin
const origin       = window.location.origin; // e.g., "https://ballsy.onrender.com"
const API_BASE_URL = origin;                 // ‚Äú/api/‚Ä¶‚Äù will be relative to this
// We are not using WebSockets in this file, but kept here for reference in case you switch to WS:
const WS_BASE_URL  = origin.startsWith("https://")
    ? "wss://" + window.location.host
    : "ws://"  + window.location.host;

// State
let speechRecognition      = null;
let speechSynthesis        = window.speechSynthesis;
let isListening            = false;
let isInitializing         = false;
let isSpeaking             = false;
let speechTimeout          = null;
let lastClickTime          = 0;
let clickHandlerAttached   = false;

// Voice settings (matching your backend defaults)
let voiceSettings = {
    voice: 'Samantha',
    rate:  1.0,
    pitch: 1.0,
    volume:1.0
};


/**
 * Initialize text-to-speech
 * Loads and caches available voices.
 */
function initTextToSpeech() {
    if (!speechSynthesis) {
        console.warn('‚ùå Text-to-speech not supported');
        return false;
    }

    // Load voices; Chrome may load them asynchronously
    let voices = speechSynthesis.getVoices();
    if (voices.length === 0) {
        speechSynthesis.onvoiceschanged = () => {
            voices = speechSynthesis.getVoices();
            console.log('üîä Voices loaded:', voices.length);
        };
    } else {
        console.log('üîä Voices already available:', voices.length);
    }

    console.log('üîä Text-to-speech initialized');
    return true;
}


/**
 * Speak text using browser's text-to-speech API
 */
function speakText(text) {
    if (!speechSynthesis || isSpeaking) {
        console.log('Speech synthesis not available or already speaking');
        return;
    }

    // Cancel any ongoing speech
    speechSynthesis.cancel();

    // Create utterance
    const utterance = new SpeechSynthesisUtterance(text);

    // Select a voice (prefer ‚ÄúDaniel‚Äù if available)
    const voices = speechSynthesis.getVoices();
    const voice = voices.find(v => v.name.includes('Daniel')) ||
                  voices.find(v => v.name.includes('Male'))    ||
                  voices.find(v => v.lang.startsWith('en-'))  ||
                  voices[0];

    if (voice) {
        utterance.voice = voice;
    }
    utterance.rate   = voiceSettings.rate;
    utterance.pitch  = voiceSettings.pitch;
    utterance.volume = voiceSettings.volume;

    // Event handlers
    utterance.onstart = () => {
        console.log('üîä Started speaking:', text);
        isSpeaking = true;
        updateUIStateFallback('speaking');
    };
    utterance.onend = () => {
        console.log('üîä Finished speaking');
        isSpeaking = false;
        updateUIStateFallback('idle');
    };
    utterance.onerror = (event) => {
        console.error('üîä Speech error:', event.error);
        isSpeaking = false;
        updateUIStateFallback('idle');
    };

    // Speak
    speechSynthesis.speak(utterance);
}


/**
 * Initialize browser speech recognition
 * Uses one-shot (non-continuous) mode so it only fires once per spoken phrase.
 */
function initSpeechRecognition() {
    if (speechRecognition) {
        console.log('üé§ Speech recognition already initialized');
        return true;
    }

    console.log('üé§ Initializing speech recognition...');

    if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
        console.error('‚ùå Speech recognition not supported');
        return false;
    }

    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    speechRecognition = new SpeechRecognition();

    // CONFIGURE for one-shot recognition:
    speechRecognition.continuous      = false;  // listen once, then end
    speechRecognition.interimResults  = false;  // only final result
    speechRecognition.lang            = 'en-US';
    speechRecognition.maxAlternatives = 1;

    // When recognition starts:
    speechRecognition.onstart = function() {
        console.log('üé§ Speech recognition started (one-shot)');
        isListening    = true;
        isInitializing = false;
        updateUIStateFallback('listening');
    };

    // Only one final result will fire when you pause:
    speechRecognition.onresult = function(event) {
        // event.results[0][0] is the full transcript of your speech until pause
        const transcript = event.results[0][0].transcript.trim();
        console.log('üéØ Full transcript:', transcript);

        // Send the entire phrase as a command immediately
        addMessageToConversationFallback('user', transcript);
        updateUIStateFallback('processing');
        sendCommandFallback(transcript);
    };

    speechRecognition.onerror = function(event) {
        console.error('‚ùå Speech recognition error:', event.error);
        isListening    = false;
        isInitializing = false;

        let message = 'I couldn\'t start listening. Please try again.';
        switch (event.error) {
            case 'no-speech':
                message = 'I didn\'t hear anything. Please try again.';
                break;
            case 'audio-capture':
                message = 'Microphone access denied. Please allow microphone access.';
                break;
            case 'not-allowed':
                message = 'Microphone permission required. Please allow and refresh the page.';
                break;
            case 'network':
                message = 'Network error. Please check your internet connection.';
                break;
            case 'aborted':
                console.log('Speech recognition aborted by user');
                updateUIStateFallback('idle');
                return;
        }

        addMessageToConversationFallback('assistant', message);
        speakText(message);
        updateUIStateFallback('idle');
    };

    // When recognition ends (either after onresult or error), do NOT restart automatically
    speechRecognition.onend = function() {
        console.log('üõë Speech recognition ended (one-shot).');
        isListening    = false;
        isInitializing = false;
        updateUIStateFallback('idle');

        // We removed the automatic restart here so Ballsy stops until user clicks again.
        // If you ever want to re-enable continuous listening, uncomment the next line:
        // speechRecognition.start();
    };

    console.log('‚úÖ Speech recognition initialized successfully');
    return true;
}


/**
 * Start listening with proper state management
 */
async function startListening() {
    const now = Date.now();
    // Debounce repeated clicks
    if (now - lastClickTime < CLICK_DEBOUNCE) {
        console.log('‚ö†Ô∏è Click debounced, too soon');
        return;
    }
    lastClickTime = now;

    console.log('üé§ startListening (one-shot) called');

    // If currently speaking, cancel that
    if (isSpeaking) {
        speechSynthesis.cancel();
        isSpeaking = false;
    }

    // Already listening or initializing?
    if (isListening || isInitializing) {
        console.log('‚ö†Ô∏è Already listening or initializing');
        return;
    }

    // Initialize if needed
    if (!speechRecognition && !initSpeechRecognition()) {
        console.log('‚ùå Could not initialize speech recognition');
        return;
    }

    try {
        isInitializing = true;
        console.log('üéØ Starting speech recognition...');
        speechRecognition.start();
    } catch (error) {
        console.error('‚ùå Error starting speech recognition:', error);
        isInitializing = false;
        isListening    = false;
        const errorMsg = 'I couldn\'t start listening. Please try again.';
        addMessageToConversationFallback('assistant', errorMsg);
        speakText(errorMsg);
        updateUIStateFallback('idle');
    }
}


/**
 * Stop listening
 */
function stopListening() {
    console.log('üõë stopListening called');

    if (speechRecognition && (isListening || isInitializing)) {
        try {
            speechRecognition.stop();
        } catch (error) {
            console.error('Error stopping speech recognition:', error);
        }
    }

    clearTimeout(speechTimeout);
    isListening    = false;
    isInitializing = false;
    updateUIStateFallback('idle');
}


/**
 * Send command to backend
 * Uses dynamic API_BASE_URL instead of hardcoded localhost
 */
async function sendCommandFallback(command) {
    console.log('üì§ Sending command to Ballsy:', command);

    try {
        const response = await fetch(`${API_BASE_URL}/api/command`, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json'
            },
            body: JSON.stringify({
                command: command,
                user_id:  1
            })
        });

        if (!response.ok) {
            throw new Error(`HTTP ${response.status}`);
        }

        const data = await response.json();
        console.log('‚úÖ Ballsy response:', data);

        // Display Ballsy‚Äôs response in the conversation UI
        addMessageToConversationFallback('assistant', data.response);

        // Speak the response
        speakText(data.response);

        // Handle ‚Äúopen_url‚Äù action if provided
        if (data.action === 'open_url' && data.data?.url) {
            window.open(data.data.url, '_blank');
        }

        // No need to force‚Äêstop or restart recognition here‚Äî
        // onend will take over once speakText finishes.

    } catch (error) {
        console.error('‚ùå Error sending command:', error);
        const errorMsg = 'Sorry, I had an error. Please try again.';
        addMessageToConversationFallback('assistant', errorMsg);
        speakText(errorMsg);
        updateUIStateFallback('idle');
    }
}


/**
 * Add message to conversation UI (fallback)
 * If main appFunctions.addMessageToConversation is available, use that; otherwise, manually append.
 */
function addMessageToConversationFallback(sender, text) {
    console.log(`üí¨ ${sender}: ${text}`);

    if (window.appFunctions && window.appFunctions.addMessageToConversation) {
        window.appFunctions.addMessageToConversation(sender, text);
        return;
    }

    const conversationHistory = document.getElementById('conversation-history');
    if (conversationHistory) {
        const messageElement = document.createElement('div');
        messageElement.classList.add('message', `${sender}-message`);
        messageElement.textContent = text;
        conversationHistory.appendChild(messageElement);
        conversationHistory.scrollTop = conversationHistory.scrollHeight;
    }
}


/**
 * Update UI state (fallback)
 * Tries window.appFunctions.updateUIState first; otherwise, updates the orb/status manually.
 */
function updateUIStateFallback(state) {
    console.log('üé® UI state:', state);

    if (window.appFunctions && window.appFunctions.updateUIState) {
        window.appFunctions.updateUIState(state);
        return;
    }

    const voiceOrb = document.querySelector('.voice-orb');
    if (voiceOrb) {
        voiceOrb.classList.remove('idle', 'listening', 'processing', 'speaking', 'error');
        voiceOrb.classList.add(state);
    }

    const statusText = document.getElementById('status-text');
    if (statusText) {
        switch (state) {
            case 'idle':
                statusText.textContent = 'Click to activate';
                break;
            case 'listening':
                statusText.textContent = 'Listening...';
                break;
            case 'processing':
                statusText.textContent = 'Processing...';
                break;
            case 'speaking':
                statusText.textContent = 'Speaking...';
                break;
            case 'error':
                statusText.textContent = 'Error occurred';
                break;
        }
    }
}


/**
 * Setup click handler for the voice orb
 */
function setupVoiceOrbClickHandler() {
    if (clickHandlerAttached) {
        console.log('Click handler already attached');
        return;
    }

    const voiceOrb = document.querySelector('.voice-orb');
    if (!voiceOrb) {
        console.log('Voice orb not found');
        return;
    }

    // Remove any existing listeners
    voiceOrb.removeEventListener('click', handleVoiceOrbClick);

    // Add new listener
    voiceOrb.addEventListener('click', handleVoiceOrbClick, { passive: true });
    clickHandlerAttached = true;

    console.log('‚úÖ Ballsy voice orb click handler attached');
}


/**
 * Handle voice orb click
 * Toggling between listening, speaking, and idle states
 */
function handleVoiceOrbClick(event) {
    event.preventDefault();
    event.stopPropagation();

    console.log('üé§ Ballsy voice orb clicked');

    if (isListening || isInitializing) {
        console.log('Stopping current listening session');
        stopListening();
    } else if (isSpeaking) {
        console.log('Stopping speech synthesis');
        speechSynthesis.cancel();
        isSpeaking = false;
        updateUIStateFallback('idle');
    } else {
        console.log('Starting new listening session');
        startListening();
    }
}


// Expose key functions globally so other scripts (app.js, ui.js) can call them
window.voiceFunctions = {
    startListening,
    stopListening,
    speakText,
    initSpeechRecognition,
    initTextToSpeech,
    setupVoiceOrbClickHandler
};

window.startListening = startListening;
window.stopListening  = stopListening;
window.speakText      = speakText;


// ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

console.log('üé§ Ballsy Voice Assistant (voice.js) loaded successfully');

// Initialize speech recognition and synthesis when the DOM is ready
document.addEventListener('DOMContentLoaded', () => {
    console.log('üé§ DOM ready - initializing Ballsy voice features');

    initSpeechRecognition();
    initTextToSpeech();

    // Attach click handler to the orb after a short delay (allow DOM to settle)
    setTimeout(() => {
        setupVoiceOrbClickHandler();
    }, 500);

    // Update the header title to show "Ballsy" instead of generic "Voice Assistant"
    const title = document.querySelector('h1');
    if (title && title.textContent.includes('Voice Assistant')) {
        title.textContent = 'Ballsy - Voice Assistant';
    }
});

console.log('üé§ Ballsy Voice Assistant script initialization complete');
